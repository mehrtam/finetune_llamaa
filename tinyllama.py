# -*- coding: utf-8 -*-
"""TinyLlama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fxglyh3ruCZSxrJYfDTacFKGzVhUehCG
"""

!pip install -q transformers accelerate peft datasets bitsandbytes sentencepiece

from huggingface_hub import notebook_login
notebook_login()

!pip install transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",           # Automatically move model to GPU
    torch_dtype="auto"           # Use appropriate precision (fp16 if supported)
)

#  Generate a sample output
input_text = "Once upon a time"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

output = model.generate(**inputs, max_new_tokens=30)
print(tokenizer.decode(output[0]))

!pip install -U datasets fsspec

import shutil
shutil.rmtree("/root/.cache/huggingface", ignore_errors=True)
shutil.rmtree("/root/.cache/fsspec", ignore_errors=True)

from datasets import load_dataset

dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
train_data = dataset["train"].filter(lambda x: len(x["text"].strip()) > 30)

def tokenize(example):
    result = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )
    result["labels"] = result["input_ids"].clone()
    return result

train_data = train_data.map(tokenize, batched=True, remove_columns=["text"])

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir="./llama-peft",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    logging_steps=10,
    save_steps=100,
    fp16=True,
    report_to="none",
)


data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

model.save_pretrained("llama_peft_lora")
tokenizer.save_pretrained("llama_peft_lora")

!zip -r llama_peft_lora.zip llama_peft_lora

from google.colab import files
files.download("llama_peft_lora.zip")


from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("llama_peft_lora")

model = PeftModel.from_pretrained(base_model, "llama_peft_lora")
model.eval()

import torch

input_text = "The history of artificial intelligence"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.8,
        top_k=50
    )

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

import torch
from datasets import load_dataset
import random

# Load dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
eval_data = dataset["test"].filter(lambda x: len(x["text"].strip()) > 30)

# Accuracy accumulators
top1_total, top3_total, top5_total = 0, 0, 0
runs = 10
samples_per_run = 30

for run in range(runs):
    print(f"\n Run {run + 1}:\n" + "="*30)

    samples = random.sample(list(eval_data), samples_per_run)
    top1_correct = top3_correct = top5_correct = total = 0

    for i, sample in enumerate(samples):
        text = sample["text"].strip()
        input_ids = tokenizer.encode(text, return_tensors="pt").to(model.device)

        if input_ids.shape[1] < 2:
            continue  # skip short input

        with torch.no_grad():
            outputs = model(input_ids[:, :-1])
            logits = outputs.logits  # shape: (batch, seq_len, vocab_size)

        next_token_logits = logits[0, -1]  # last token's logits
        topk = torch.topk(next_token_logits, k=5)
        topk_ids = topk.indices.tolist()
        true_id = input_ids[0, -1].item()

        is_top1 = true_id == topk_ids[0]
        is_top3 = true_id in topk_ids[:3]
        is_top5 = true_id in topk_ids

        top1_correct += int(is_top1)
        top3_correct += int(is_top3)
        top5_correct += int(is_top5)
        total += 1

    run_top1 = top1_correct / total
    run_top3 = top3_correct / total
    run_top5 = top5_correct / total

    print(f"Top-1 Accuracy: {top1_correct}/{total} = {run_top1 * 100:.2f}%")
    print(f"Top-3 Accuracy: {top3_correct}/{total} = {run_top3 * 100:.2f}%")
    print(f"Top-5 Accuracy: {top5_correct}/{total} = {run_top5 * 100:.2f}%")

    top1_total += run_top1
    top3_total += run_top3
    top5_total += run_top5

# Final average
print("\nFinal Averaged Results over 10 runs (30 samples each):")
print("----------------------------------------------------------")
print(f"Average Top-1 Accuracy: {top1_total / runs * 100:.2f}%")
print(f"Average Top-3 Accuracy: {top3_total / runs * 100:.2f}%")
print(f"Average Top-5 Accuracy: {top5_total / runs * 100:.2f}%")

import torch

# 1. Define 30 custom test samples (prompt + expected next token)
custom_samples = [
    ("The sun rises in the", "morning"),
    ("Artificial intelligence is transforming the", "world"),
    ("He walked into the room and saw a", "man"),
    ("This new policy could improve public", "health"),
    ("The patient was diagnosed with a rare", "disease"),
    ("Quantum computing could change the future of", "technology"),
    ("Her favorite book is about ancient", "history"),
    ("The cat jumped on the", "table"),
    ("He forgot to bring his", "keys"),
    ("Many people are afraid of the", "dark"),
    ("She loves to drink coffee in the", "morning"),
    ("The capital of France is", "Paris"),
    ("The internet was invented in the", "1960s"),
    ("Neural networks are inspired by the human", "brain"),
    ("Photosynthesis occurs in the", "leaves"),
    ("Electric vehicles are powered by", "batteries"),
    ("The moon orbits around the", "Earth"),
    ("Python is a popular programming", "language"),
    ("He looked at her and smiled with", "joy"),
    ("They decided to go for a long", "walk"),
    ("The temperature dropped below", "zero"),
    ("In biology, cells are the basic unit of", "life"),
    ("She opened the door and found a", "package"),
    ("The birds are singing in the", "trees"),
    ("He was awarded the Nobel", "Prize"),
    ("The experiment was a complete", "success"),
    ("A rainbow appears after the", "rain"),
    ("The dog barked at the", "stranger"),
    ("The movie received critical", "acclaim"),
    ("The scientist presented her latest", "findings"),
]

# 2. Initialize counters
top1_correct = 0
top3_correct = 0
top5_correct = 0
total = 0

# 3. Evaluate each sample
for i, (prompt, expected_word) in enumerate(custom_samples):
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits
        next_token_logits = logits[0, -1]  # (vocab_size,)
        topk = torch.topk(next_token_logits, k=5)
        topk_ids = topk.indices.tolist()
        topk_tokens = [tokenizer.decode([tok]).strip() for tok in topk_ids]

    expected_token_id = tokenizer.encode(expected_word, add_special_tokens=False)
    if not expected_token_id:
        print(f"[{i+1}] Token not found for: '{expected_word}'")
        continue
    expected_token_id = expected_token_id[0]

    is_top1 = expected_token_id == topk_ids[0]
    is_top3 = expected_token_id in topk_ids[:3]
    is_top5 = expected_token_id in topk_ids

    top1_correct += int(is_top1)
    top3_correct += int(is_top3)
    top5_correct += int(is_top5)
    total += 1

    print(f"[{i+1}] Prompt: '{prompt}' â†’ Expected: '{expected_word}'")
    print(f"     Top-5 Predictions: {topk_tokens}")
    print(f"     Top-1: {is_top1} | Top-3: {is_top3} | Top-5: {is_top5}\n")

# 4. Final accuracy
print("====================================")
print(f"Top-1 Accuracy: {top1_correct}/{total} = {top1_correct / total * 100:.2f}%")
print(f"Top-3 Accuracy: {top3_correct}/{total} = {top3_correct / total * 100:.2f}%")
print(f"Top-5 Accuracy: {top5_correct}/{total} = {top5_correct / total * 100:.2f}%")



# from huggingface_hub import login
# login()

# from huggingface_hub import login
# from transformers import AutoModelForCausalLM, AutoTokenizer

# # Step 1: Log in if you haven't already (you'll be asked for a token)
# login()

# # Step 2: Push the model and tokenizer
# model.push_to_hub("mehrta/llama_peft_lora")
# tokenizer.push_to_hub("mehrta/llama_peft_lora")

